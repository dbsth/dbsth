# main_crawler.py (ì‹¤í–‰ ë¡œì§)
import os
import time
from datetime import datetime

from crawler_config import ConfigLoader
from crawler_utils import CrawlerUtils
from page_processor import PageProcessor
from url_builder import URLBuilder
from data_handler import DataHandler
from date_formatter import DateFormatter
from load_csv import load_latest_csv_as_dataframe
from post_crawler import PostExtractor
from file_manager import FileManager
from urllib.parse import urlparse, parse_qs
import pandas as pd
import re
from bs4 import BeautifulSoup

class NoticeCrawler:
    def __init__(self, config_path, csv_save_directory="data"):
        self.url_list = ConfigLoader.load_urls(config_path)
        self.collected_data = []
        self.csv_save_directory = csv_save_directory
        self.existing_df = self._load_existing_dataframe() # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ ë¡œë“œ
        self.post_extractor = PostExtractor() # PostExtractor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

    def _load_existing_dataframe(self):
        """ê¸°ì¡´ì˜ ìµœì‹  CSV íŒŒì¼ì„ ë¡œë“œí•˜ê±°ë‚˜, íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        output_dir = os.path.join(os.path.dirname(__file__), "..", self.csv_save_directory)
        try:
            return load_latest_csv_as_dataframe(output_dir)
        except FileNotFoundError:
            print("âš ï¸ ê¸°ì¡´ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ DataFrameìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            return pd.DataFrame()
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def process_site(self, site):
        """crawler_url.jsonì— ì •ì˜ëœ ê° ì‚¬ì´íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ê²Œì‹œë¬¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        base_url = site.get("url", "")
        board_path = site.get("board_path", "")
        crawl_delay = site.get("crawl_delay", 5)
        timeout = site.get("crawl_timeout", 30)
        page = 0

        print(f"\nğŸ” {base_url} í¬ë¡¤ë§ ì‹œì‘...")

        while True:
            current_url = URLBuilder.build_paginated_url(base_url, page)
            print(f"    ğŸ”„ [{page + 1}í˜ì´ì§€] ìš”ì²­ ì¤‘...")

            try:
                html_content = CrawlerUtils.make_request(current_url, timeout)
                processor = PageProcessor(html_content)

                if not processor.has_content():
                    print(f"âš ï¸ [{page + 1}í˜ì´ì§€] ì»¨í…ì¸  ì—†ìŒ")
                    break

                posts = processor.extract_posts(board_path)
                newly_collected = self._process_posts(posts, base_url, site)

                if not newly_collected:
                    print(f"â›” í˜ì´ì§€ {page + 1}ì—ì„œ ìƒˆë¡œìš´ ê²Œì‹œë¬¼ ì—†ìŒ. í•´ë‹¹ URL í¬ë¡¤ë§ ì™„ë£Œ.")
                    break

                self.collected_data.extend(newly_collected)
                page += 1
                time.sleep(crawl_delay)

            except Exception as e:
                print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
                break

    def _process_posts(self, posts, base_url, site):
        """ê²Œì‹œë¬¼ ëª©ë¡ì—ì„œ ê° ê²Œì‹œë¬¼ì˜ ë§í¬ë¥¼ ë”°ë¼ê°€ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ì¤‘ë³µì„ í™•ì¸í•©ë‹ˆë‹¤."""
        newly_collected_posts = []
        new_post_found = False
        duplicate_check_key = site.get("duplicate_check_key", "articleNo")

        # collected_data ì¤‘ë³µ í™•ì¸ì„ ìœ„í•œ set (íŠœí”Œ í˜•íƒœë¡œ ì €ì¥í•˜ì—¬ í•´ì‹± ê°€ëŠ¥í•˜ê²Œ í•¨)
        collected_data_keys = set([(item.get(duplicate_check_key), item.get("university"), item.get("department"))
                                     for item in self.collected_data if item.get(duplicate_check_key)])

        # existing_df ì¤‘ë³µ í™•ì¸ì„ ìœ„í•œ set (íŠœí”Œ í˜•íƒœë¡œ ì €ì¥)
        existing_df_keys = set()
        if not self.existing_df.empty and duplicate_check_key in self.existing_df.columns and \
           "university" in self.existing_df.columns and "department" in self.existing_df.columns:
            existing_df_keys = set(zip(self.existing_df[duplicate_check_key].astype(str),
                                       self.existing_df["university"].astype(str),
                                       self.existing_df["department"].astype(str)))

        for post_element in posts:
            title = "".join(post_element.stripped_strings).strip()
            href = post_element.get("href")

            full_url = CrawlerUtils.get_full_url(base_url, href)

            post_data = self._crawl_and_extract_post(full_url, site)
            if post_data:
                duplicate_key = post_data.get(duplicate_check_key)
                post_university = post_data.get("university")
                post_department = post_data.get("department")

                if duplicate_key:
                    # collected_data ì¤‘ë³µ í™•ì¸
                    current_key = (duplicate_key, post_university, post_department)
                    if current_key in collected_data_keys:
                        # print(f"âš ï¸ ì´ë²ˆ ìˆ˜ì§‘ ë°ì´í„° ì¤‘ ì¤‘ë³µ ê²Œì‹œë¬¼ ë°œê²¬ ({duplicate_check_key}={duplicate_key}): {title}")
                        continue

                    # existing_df ì¤‘ë³µ í™•ì¸
                    if (str(duplicate_key), str(post_university), str(post_department)) in existing_df_keys:
                        # print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ì¤‘ ì¤‘ë³µ ê²Œì‹œë¬¼ ë°œê²¬ ({duplicate_check_key}={duplicate_key}): {title}")
                        continue

                    newly_collected_posts.append(post_data)
                    collected_data_keys.add(current_key) # ìƒˆë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í‚¤ ì¶”ê°€
                    new_post_found = True
                    # print(f"ğŸ“Œ ìƒˆ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ: {title} ({full_url})")

        if new_post_found:
            return newly_collected_posts
        else:
            return None

    def _crawl_and_extract_post(self, post_url, site):
        """ì£¼ì–´ì§„ URLì˜ ê²Œì‹œë¬¼ ìƒì„¸ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            response = CrawlerUtils.make_request(post_url)
            return self.post_extractor.extract_post_data(post_url, response, site)
        except Exception as e:
            print(f"âŒ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì‹¤íŒ¨: {post_url} - {str(e)}")
            return None

    def run(self):
        """í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        for site in self.url_list:
            self.process_site(site)

        all_collected_df = pd.DataFrame(self.collected_data)

        if not all_collected_df.empty:
            # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ê³¼ ìƒˆë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë³‘í•©
            final_df = pd.concat([self.existing_df, all_collected_df], ignore_index=True)
        else:
            final_df = self.existing_df # ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ ìœ ì§€

        output_dir = os.path.join(os.path.dirname(__file__), "..", self.csv_save_directory)
        os.makedirs(output_dir, exist_ok=True)
        csv_filename = DateFormatter.get_timestamp_filename()
        output_path = os.path.join(output_dir, csv_filename)

        try:
            final_df.to_csv(output_path, index=False, encoding="utf-8-sig")
            print(f"\nâœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        except Exception as e:
            print(f"âŒ CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

        # ì˜¤ë˜ëœ CSV íŒŒì¼ ì‚­ì œ (í˜„ì¬ëŠ” ë™ì‘í•˜ì§€ ì•ŠìŒ)
        # FileManager.delete_old_csv_files(output_dir, days_threshold=7)

if __name__ == "__main__":
    crawler = NoticeCrawler("test_crawler_url.json")
    crawler.run()