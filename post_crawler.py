# web_crawler/post_extractor.py
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
import re
from crawler_utils import CrawlerUtils

class PostExtractor:
    def __init__(self, base_url="https://cse.kangwon.ac.kr"):
        self.base_url = base_url

    def extract_article_no(self, url, site=None):
        """URLì—ì„œ ê²Œì‹œë¬¼ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. site ì •ë³´ì—ì„œ íŒŒë¼ë¯¸í„° í‚¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
        parsed = urlparse(url)
        query = parse_qs(parsed.query)
        article_id_param = site.get("article_id_param", "articleNo") if site else "articleNo"
        return query.get(article_id_param, [None])[0]

    def extract_post_data(self, post_url, html_content, site):
        """HTML ë‚´ìš©ê³¼ ì‚¬ì´íŠ¸ ì„¤ì •ì—ì„œ ê²Œì‹œë¬¼ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            soup = BeautifulSoup(html_content, "html.parser")

            title_selector = site.get("title_path")
            title_tag = soup.select_one(title_selector)
            title = title_tag.get_text(strip=True) if title_tag else None
            # print(f"    ğŸ“œ ì œëª©: {title}")

            writer_selector = site.get("writer_path")
            writer_tag = soup.select_one(writer_selector)
            author = writer_tag.text.strip() if writer_tag else None
            # print(f"    âœï¸ ì‘ì„±ì: {author}")

            date_selector = site.get("date_path")
            date_tag = soup.select_one(date_selector)
            date = date_tag.text.strip() if date_tag else None
            # print(f"    ğŸ“… ë‚ ì§œ: {date}")
            
            content_selector = site.get("content_path")
            content_tag = soup.select_one(content_selector)
            if content_tag:
                content = re.sub(r"\s+", " ", content_tag.text.strip())  # ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                content = content.replace("\u00a0", " ")  # `NBSP` ì œê±°
            else:
                content = "no content"
            # print(f"    ğŸ“„ ë‚´ìš©: {content[:10]}")

            attachment_selector = site.get("attachment_path")
            attachment_tags = soup.select(attachment_selector)
            attachments = [CrawlerUtils.get_full_url(self.base_url, a['href'])
                           for a in attachment_tags if a.has_attr('href')]
            # print(f"    ğŸ“ ì²¨ë¶€íŒŒì¼: {attachments}")

            articleNo = self.extract_article_no(post_url, site)

            return {
                "title": title,
                "date": date,
                "author": author,
                "articleNo": articleNo,
                "content": content,
                "link": post_url,
                "attachments": attachments,
                "university": site.get('university'),
                "department": site.get('department')
            }
        except Exception as e:
            print(f"âŒ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {post_url} - {str(e)}")
            return None